========
OVERVIEW
========

The main classes in the package and their functions are listed below:

* Solver        Implements the core of the algorithm.
* Model         Template for a problem specification.
* ILowerBound   Interface for a lower bound strategy.
* IUpperBound   Interface for an upper bound strategy.
* BeliefUpdate  Abstract class for a belief-update strategy.
* VNode         A belief node in the search tree.
* QNode         An intermediate (AND) node in the search tree.
* World         Maintains and updates the true state of the world.

The following lower bound implementations are provided:

* PolicyLowerBound: Computes the lower bound by following a policy from a belief
  |                 node (set of particles). An action is selected and  applied
  |                 to all particles. The resulting particles are branched at 
  |                 the observation node and the simulation is carried out 
  |                 recursively at the next level. There are 2 specializations
  |                 of this class that differ in how the action is selected:
  |
  +---> ModePolicyLowerBound: selects the best action for the mode of the
  |                           particle set.
  |
  +---> RandomPolicyLowerBound: selects a random action.

The following upper bound implementations are provided:

* UpperBoundStochastic:    Computes the upper bound for models that have 
                           non-deterministic state transitions. This is the
                           general case for a POMDP.
* UpperBoundNonStochastic: Computes the upper bound for models that have
                           deterministic state transitions. This is a special 
                           case of the stochastic version with a minor
                           difference and better time/space complexity.

The following belief-update implementations are provided:

* ParticleFilterUpdate: Implements sequential importance resampling.
                        It is specialized for Pocman, and for Tag for reasons
                        explained in the source code.
* ExactBeliefUpdate:    Maintains and updates a probability distribution
                        over states. This is not feasible for problems with
                        large state spaces, like RockSample(15, 15) and
                        Pocman.

More details about each of these components are given in the source code.

Solver is the central module that maintains references to the lower(upper)
bound, belief update, and the user-defined model, tying them together.
The lower(upper) bound and belief-update modules require the model to 
implement certain methods, which are marked appropriately in the model
template. A class diagram depicting the arrangement can be found at 
`despot/doc/Class_Diagram.png`.


===
FAQ
===

* Why make the model base class a template?

A straightforward way of abstracting the model is to have a "Model" interface
that is implemented by each new problem, and use a Model* type in the algorithm
to achieve polymorphism. Instead, each problem gets its own superclass of
type Model<Problem-State-Type> (see src/model.h).

If Model were a simple class, we would need a "State" abstract class for some
of Model's methods. For example,

class IModel {
  void step(State* s, int action);

  bool isTerminal(const State* s);
};

A new problem would have to use the same method signature and override it like
this:

class Tag : public IModel {
  void step(State* s, int action) {
    TagState* state = static_cast<TagState*>(s);
    // Do something with state
    ...
  }

  bool isTerminal(const State* s) {
    TagState* state = static_cast<TagState*>(s);
    // Do something with state
    ...
  }
};
 
For simple problems like Tag, stepping a state or checking terminal condition
is not a very expensive operation, which means that the relative overhead of 
the downcast is significant. Moreover, these methods are called a huge number
of times, so the total overhead of the cast is also significant. In practice,
the slowdown was observed to be 3-4x.

Making Model a template with problem-specific state as parameter allows us
to circumvent this:

template<typename T>
class Model {
  void step(T& s, int action);
  bool isTerminal(const T& s);
};

class TagState { ... };

class Model<TagState> {
  void step(TagState& state, int action) {
    // No downcast needed
    ...
  }
  ...
};

In essence, we use compile-time polymorphism instead of runtime polymorphism
for efficiency. Another advantage of using templates is that it makes working
with states easier because we can work directly with the true state types
instead of manipulating State* pointers that not only need repeated casting,
but also require careful memory management.

The disadvantage of templates is that it causes a cascade effect that requires
many other classes to be templated as well. Another side effect is that the
compiler will complain if we separate the interface and implementation of 
template definitions. One way of dealing with this is to put everything in
the .h files, which is the chosen method in our implementation.

Overall, using templates seems to not sacrifice design/readability too much
for the efficiency gains in simple problems.


* Why define a special terminal observation?

Currently, the model has to define a terminal observation that is emitted
in and only in a terminal state. When we take an action on a particle set, 
the particles get grouped by the observations they produce and are carried 
forward to the next belief. It is simpler to deal with the scenario where 
all terminal states get grouped together, forming a "terminal" belief node 
where the simulation stops, rather than continue the simulation on the subset
of non-terminal particles in the next belief. Having an "if and only if" 
relation between terminal states and the terminal observation is a small 
price to pay for this convenience without changing the nature of the problem.


* Instead of having lower(upper) bound and belief update as standalone modules,
  we can have Model inherit from their interfaces, and provide all
  problem-specific stuff they require via pure virtual functions 
  implemented in the model. Is this a better alternative?
  
Different lower(upper) bound implementations require different things from 
the model. For example, the policy lower bound needs to query the 
model for the best action for a given state. We could make a model inherit
from the components we wish to use. But there are a few disadvantages to
this:

- If we want a model to support multiple lower(upper) bounds / belief-update
  strategies, we need a diamond-shaped multiple inheritance structure, plus 
  override methods in the model to control which base class's methods get 
  called. These overrides cause the code to become complicated.
  
- Modules like belief-update need to change their state each time they are 
  invoked. If Model inherits from these interfaces, the otherwise const
  reference to Model maintained by the Solver will need to be non-const, 
  which is generally not a good guarantee to break.

- Some components may require the model to be instantiated before they can
  perform precomputations / initializations. Having Model inherit from these
  components means that the problem writer would be burdened with making sure
  that the order of initializations is correct, with the Model being
  initialized before the Init() method of its superclasses are called.
  A simpler alternative is to have independent modules, which take an 
  instantiated model as a parameter to their constructors.

In general, we prefer composition over inheritance, which will be beneficial
as the system grows.

The disadvantage of having standalone components is that not all components 
will be compatible with each other. For instance, exact-belief updates are not 
compatible with large problems like pocman. To be concrete, ExactBeliefUpdate
requires the problem-specific state type (e.g. PocmanState) to implement 
implicit conversions to and from int in order to be able to index it in an
array, but Pocman has too many states for this to be feasible. There are 2
options at this point:

(a) Let PocmanState be defined as it is, and ensure that incompatible components
    do not try to use each other. For example, ideally we would want to 
    instantiate the objects in our system in a generic way like this:
   
if (problem == "pocman") {
  Run(new Model<PocmanState>()); // error, see below
}
else if (problem == "tag") {
  Run(new Model<TagState>());
}

template<typename T>
void run(Model<T>* model) {
  BeliefUpdate<T>* bu;
  if (belief_update_type == "particle")
    bu = new ParticleBeliefUpdate<T>(model);
  else if (belief_update_type == "exact")
    bu = new ExactBeliefUpdate<T>(model); // Error, Pocman not compatible with 
                                          // ExactBeliefUpdate!
  ...
}

Instead, we will have to instantiation on a problem-by-problem basis:

if (problem == "pocman") {
  Model<PocmanState>* model = new Pocman();
  if (belief_update_type == "particle")
    bu = new ParticleBeliefUpdate<PocmanState>(model);
  else if (belief_update_type == "exact")
    SignalError();
  Run(model);
}

(b) The other option is to define dummy conversion operators for PocmanState
    so that from the compiler's viewpoint, the incompatible components are
    in fact compatible. We just need to write code to make sure that they are 
    never actually used together. 

if (problem == "pocman") {
  Run(new Model<PocmanState>()); // Fine
}
else if (problem == "tag") {
  Run(new Model<TagState>());
}

template<typename T>
void run(Model<T>* model) {
  BeliefUpdate<T>* bu;
  if (belief_update_type == "particle")
    bu = new ParticleBeliefUpdate<T>(model);
  else if (belief_update_type == "exact")
    if (problem == "pocman") 
      SignalError();
    else
      bu = new ExactBeliefUpdate<T>(model);
  ...
}

This makes for a lot less repetitive code in main.cpp, and is the chosen 
method in our implementation.


* Why might the agent get stuck in a state and make no progress?

This is a consequence of the algorithm rather than the design of our
implementation, but is mentioned here to keep all clarifications about tricky
aspects of the code in one place.

In the HSVI heuristic, action-selection during tree expansion depends on the 
nodes' upper bounds. In some rare cases, when the number of particles is small,
the computed upper bounds do not give a good estimate of the relative goodness
of each action. For instance, suppose that in the LaserTag problem the agent 
is in a state where its belief of the opponent's position is concentrated
in the 4 corners of the map. Regardless of whether the agent moves or stays put,
the average distance from the opponent is roughly the same. So it might be
possible that the stream of random numbers that will be seen after taking a 
single action is most favorable to the agent's current location. Coupled with
the fact that moving in any direction incurs the same immediate cost, -1, this
means that the agent will prefer moving into a wall in order to stay put. That
the move is ineffectual is domain knowledge, so the generic algorithm doesn't 
know any better. Further, since the same random number streams are used at 
each step, this is a recipe for an infinite loop. 

Note that the upper bound for a set of particles is unrelated to the upper
bound for the resulting set after taking a single action on all the particles.
This is because the upper bound is computed as the average upper bound of the 
particles, each of which may use a different first-action for its individual
upper bound. If this were not the case, the upper bound for the next set after
taking the stay-put action would presumably be lower than that of other actions
because of the discount factor. 

